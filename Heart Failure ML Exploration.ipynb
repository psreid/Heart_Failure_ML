{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4cf7b1",
   "metadata": {},
   "source": [
    "HEART FAILURE:\n",
    "Fun day-long project with the express purpose of brushing up on techniques/brushing off the rust\n",
    "This is the first installment and meant to be see how much I can get done in a day. Hope you enjoy!\n",
    "- Parker\n",
    "\n",
    " Data taken from Kaggle \"https://www.kaggle.com/andrewmvd/heart-failure-clinical-data \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38492db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of the cool toy datasets don't have much data...probably for the best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Heart_data_path = \"heart_failure_clinical_records_dataset.csv\"\n",
    "#Heart is way cooler to type than data everytime\n",
    "Heart = pd.read_csv(Heart_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets open the patient up and see the problem\n",
    "Heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we have a few binary inputs as medical conditions\n",
    "# woman is a 0 and man is a 1.\n",
    "# otherwise 0 is a no and 1 is a yes\n",
    "# ~300 rows not a terribly large dataset...\n",
    "# presumably we want sympotoms to predict death, and not vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95773271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set variables to predictors X and binary output Y\n",
    "y = Heart.iloc[:,Heart.columns ==\"DEATH_EVENT\"]\n",
    "X = Heart.iloc[:,Heart.columns !=\"DEATH_EVENT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8da3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forests are pretty clutch most of the time, let's try that out first\n",
    "#(dataset pretty small too hmm that might be a problem)\n",
    "RF = RandomForestClassifier(n_estimators=30, max_depth=9, random_state=0)\n",
    "y = np.ravel(y)\n",
    "RF.fit(X,y)\n",
    "RF.predict(X.iloc[100:,:])\n",
    "round(RF.score(X,y), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1945178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As expected, if we train on the entire dataset, and have a large depth we're going to get a perfect score.....\n",
    "# which is useless.... and I should have remembered this....\n",
    "#lets plot this effect anyway\n",
    "\n",
    "score_plot = [0 for x in range(11)]\n",
    "for i in range(11):\n",
    "    RF = RandomForestClassifier(n_estimators=30, max_depth=i+1, random_state=0)\n",
    "    y = np.ravel(y)\n",
    "    RF.fit(X,y)\n",
    "    RF.predict(X.iloc[100:,:])\n",
    "    score_plot[i] = round(RF.score(X,y), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec573fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "#Define depth as X- axis and score on Y\n",
    "plt.plot(range(1,12) , score_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth seems pretty important here, but it's definitely prone to overfitting without validation\n",
    "#but cleary this random forest is also a function of estimators,\n",
    "#curious to know what an optimal estimator with minimal depth combination is\n",
    "# lets nest a couple loops and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: this could take a few seconds, we're running the RF 250 times with varying computational budgets\n",
    "score_plot2D = [[np.nan for x in range(25)] for y in range(11)]\n",
    "for j in range(2,25):\n",
    "    for i in range(11):\n",
    "        RF = RandomForestClassifier(n_estimators=j+1, max_depth=i+1, random_state=0)\n",
    "        y = np.ravel(y)\n",
    "        RF.fit(X,y)\n",
    "        RF.predict(X.iloc[100:,:])\n",
    "        score_plot2D[i][j] = round(RF.score(X,y), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13965ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score_plot2D, aspect=\"auto\", interpolation=None, vmin=0.7, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of estimators is slightly underwhelming in this dataset. Definitely keeping it above 5 though...\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# time to do some proper data science with a real validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset this RF classifier parameters from previous test\n",
    "RF = RandomForestClassifier(n_estimators=25, max_depth=5, random_state=0)\n",
    "\n",
    "# introduce an actual training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "#Same perscription as before now\n",
    "RF.fit(train_X,train_y)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = round(cross_val_score(RF, val_X, val_y, cv=5).mean(), 2)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95758786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to see the effect of depth now when a validation set is introduced..\n",
    "score_plot_validated = [0 for x in range(11)]\n",
    "for i in range(11):\n",
    "    RF = RandomForestClassifier(n_estimators=30, max_depth=i+1, random_state=0)\n",
    "    RF.fit(train_X,train_y)\n",
    "    score_plot_validated[i] = cross_val_score(RF,val_X,val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae4f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,12) , score_plot_validated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be9b01",
   "metadata": {},
   "source": [
    "The above looks much less egregious with respect to model depth, and we have a model with an 89% correctness rate on our validation set (I'm pretty sure that's the correct interpretation in a classification model score)\n",
    "\n",
    "Let's push the model by increasing the estimators by an order of magnitude to get those juicy dimishing returns on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01235164",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=300, max_depth=5, random_state=0)\n",
    "y = np.ravel(y)\n",
    "RF.fit(train_X,train_y)\n",
    "round(cross_val_score(RF,val_X,val_y).mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb52e27",
   "metadata": {},
   "source": [
    "Looks like the statistical variance was greater than any return I'd get by increasing the number of estimators..\n",
    "Let's find some cool ways to score this and maybe adjust some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predict_y= RF.predict(val_X)\n",
    "confusion_matrix(val_y, predict_y, normalize = \"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b255a1",
   "metadata": {},
   "source": [
    " With matrix containing\n",
    " \n",
    "                   Death\t\t\tno-death\n",
    "                   \n",
    "    Death\t\ttrue positive\t\tfalse positive\n",
    "\n",
    "    no-death\tfalse negative\t\ttrue negative\n",
    "    \n",
    "It looks like the false-positive and false-negative rates are simillar to within 60%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa22ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Try to find redundant features with respect to predicting mortality?\n",
    "import seaborn\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = Heart.corr()\n",
    "seaborn.heatmap(cor, annot=True, cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfc33b",
   "metadata": {},
   "source": [
    "Nothing here stands out as a large correlation within the features. Some features correlate with Death events more weakly than others.\n",
    "\n",
    "Let's see what happens when we drop the weak correlations with death-events themselves (gee I hope the definition of correlation is normalized...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "Heart = Heart.drop(['sex', 'smoking'], axis = 1)\n",
    "Heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611db1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one more time without those columns!\n",
    "y = Heart.iloc[:,Heart.columns ==\"DEATH_EVENT\"]\n",
    "X = Heart.iloc[:,Heart.columns !=\"DEATH_EVENT\"]\n",
    "y = np.ravel(y)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "RF = RandomForestClassifier(n_estimators=300, max_depth=5, random_state=0)\n",
    "train_y = np.ravel(train_y)\n",
    "RF.fit(train_X,train_y)\n",
    "round(cross_val_score(RF,val_X,val_y).mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7122ec",
   "metadata": {},
   "source": [
    "Those two columns clearly have some decision effects somewhere. So I learned not to ignore free data....\n",
    "But in the end we have a model that works pretty well, and with time constraints I am pretty happy with the insights gained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
